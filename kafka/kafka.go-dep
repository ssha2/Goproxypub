package kafka

import (
	"bytes"
	"log"
	"time"

	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
)

type LogElem struct {
	ltype string    // response or request or error
	u_id  string    // session id
	head  []byte    // headers
	body  []byte    // body
	t     time.Time //timestamp
}

type ConfigKafka struct {
	config *kafka.ConfigMap
	topic  string
}

var configKafka ConfigKafka

// https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html
func configkafka(topicname string, brokeradr string) {
	configKafka = ConfigKafka{
		&kafka.ConfigMap{
			"bootstrap.servers":   brokeradr,
			"retries":             0,
			"delivery.timeout.ms": 1000,
			"acks":                0, //set to zero then the producer will not wait for any acknowledgment from the server at all. The record will be immediately added to the socket buffer and considered sent
		},
		topicname,
	}
}

func bytestoKafka(elem LogElem) {

	var buffer bytes.Buffer
	buffer.WriteString("$$meta$$")
	buffer.WriteString(elem.ltype)
	buffer.WriteString("\n$$id$$")
	buffer.WriteString(elem.u_id)
	buffer.WriteString("\n$$time$$")
	buffer.WriteString(elem.t.String())
	buffer.WriteString("\n$$head$$")
	buffer.Write(elem.head)
	buffer.WriteString("\n$$body$$")
	buffer.Write(elem.body)
	kafka_simle_one(buffer.Bytes(), []byte(elem.ltype))
}

func kafka_simle_one(b []byte, k []byte) {

	producer, err := kafka.NewProducer(configKafka.config)
	if err != nil {

		log.Println("ERRR")
		log.Println("Failed create producer:\n", err)
		return //silent exit
	}
	defer producer.Close()

	// Listen to all the events on the default events channel
	// go func() {
	// 	for e := range producer.Events() {
	// 		switch ev := e.(type) {
	// 		case *kafka.Message:
	// 			// The message delivery report, indicating success or
	// 			// permanent failure after retries have been exhausted.
	// 			// Application level retries won't help since the client
	// 			// is already configured to do that.
	// 			m := ev
	// 			if m.TopicPartition.Error != nil {
	// 				log.Printf("Delivery failed: %v\n", m.TopicPartition.Error)
	// 			} else {
	// 				log.Printf("Delivered message to topic %s [%d] at offset %v\n",
	// 					*m.TopicPartition.Topic, m.TopicPartition.Partition, m.TopicPartition.Offset)
	// 			}
	// 		case kafka.Error:
	// 			// Generic client instance-level errors, such as
	// 			// broker connection failures, authentication issues, etc.
	// 			//
	// 			// These errors should generally be considered informational
	// 			// as the underlying client will automatically try to
	// 			// recover from any errors encountered, the application
	// 			// does not need to take action on them.
	// 			log.Printf("Error: %v\n", ev)
	// 		default:
	// 			log.Printf("Ignored event: %s\n", ev)
	// 		}
	// 	}
	// }()

	log.Println("ORIDUCE")
	err = producer.Produce(&kafka.Message{
		TopicPartition: kafka.TopicPartition{Topic: &configKafka.topic, Partition: kafka.PartitionAny},
		Value:          b,
		Key:            k,
	}, nil)
	if err != nil {
		log.Println("Failed to produce message:\n", err)
	} else {
		// Flush and close the producer and the events channel
		for producer.Flush(1000) > 0 {

			log.Println("FLUSH")
			//just wait
		}
	}


}
